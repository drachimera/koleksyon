{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original tutorial: https://docs.pymc.io/notebooks/api_quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m102417/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "\n",
    "Models in PyMC3 are centered around the Model class. It has references to all random variables (RVs) and computes the model logp and its gradients. Usually, you would instantiate it as part of a with context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Model definition\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss RVs further below but let’s create a simple model to explore the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mu, obs]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mu]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.free_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[obs]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.observed_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-143.08159657)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logp({'mu': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning It’s worth highlighting one of the counter-intuitive design choices with logp. The API makes the logp look like an attribute, when it actually puts together a function based on the current state of the model.\n",
    "\n",
    "The current design is super maintainable, does terrible if the state stays constant, and great if the state keeps changing, for reasons of design we assume that Model isn’t static, in fact it’s best in our experience and avoids bad results.\n",
    "\n",
    "If you need to use logp in an inner loop and it needs to be static, simply use something like logp = model.logp below. You can see the caching effect with the speed up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.1 ms ± 2.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "17.8 µs ± 1.43 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.logp({mu: 0.1})\n",
    "logp = model.logp\n",
    "%timeit logp({mu: 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions¶\n",
    "Every probabilistic program consists of observed and unobserved Random Variables (RVs). Observed RVs are defined via likelihood distributions, while unobserved RVs are defined via prior distributions. In PyMC3, probability distributions are available from the main module space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Normal in module pymc3.distributions.continuous:\n",
      "\n",
      "class Normal(pymc3.distributions.distribution.Continuous)\n",
      " |  Univariate normal log-likelihood.\n",
      " |  \n",
      " |  The pdf of this distribution is\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     f(x \\mid \\mu, \\tau) =\n",
      " |         \\sqrt{\\frac{\\tau}{2\\pi}}\n",
      " |         \\exp\\left\\{ -\\frac{\\tau}{2} (x-\\mu)^2 \\right\\}\n",
      " |  \n",
      " |  Normal distribution can be parameterized either in terms of precision\n",
      " |  or standard deviation. The link between the two parametrizations is\n",
      " |  given by\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     \\tau = \\dfrac{1}{\\sigma^2}\n",
      " |  \n",
      " |  .. plot::\n",
      " |  \n",
      " |      import matplotlib.pyplot as plt\n",
      " |      import numpy as np\n",
      " |      import scipy.stats as st\n",
      " |      plt.style.use('seaborn-darkgrid')\n",
      " |      x = np.linspace(-5, 5, 1000)\n",
      " |      mus = [0., 0., 0., -2.]\n",
      " |      sigmas = [0.4, 1., 2., 0.4]\n",
      " |      for mu, sigma in zip(mus, sigmas):\n",
      " |          pdf = st.norm.pdf(x, mu, sigma)\n",
      " |          plt.plot(x, pdf, label=r'$\\mu$ = {}, $\\sigma$ = {}'.format(mu, sigma))\n",
      " |      plt.xlabel('x', fontsize=12)\n",
      " |      plt.ylabel('f(x)', fontsize=12)\n",
      " |      plt.legend(loc=1)\n",
      " |      plt.show()\n",
      " |  \n",
      " |  ========  ==========================================\n",
      " |  Support   :math:`x \\in \\mathbb{R}`\n",
      " |  Mean      :math:`\\mu`\n",
      " |  Variance  :math:`\\dfrac{1}{\\tau}` or :math:`\\sigma^2`\n",
      " |  ========  ==========================================\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  mu : float\n",
      " |      Mean.\n",
      " |  sigma : float\n",
      " |      Standard deviation (sigma > 0) (only required if tau is not specified).\n",
      " |  tau : float\n",
      " |      Precision (tau > 0) (only required if sigma is not specified).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, sigma=10)\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, tau=1/23)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normal\n",
      " |      pymc3.distributions.distribution.Continuous\n",
      " |      pymc3.distributions.distribution.Distribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mu=0, sigma=None, tau=None, sd=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  logcdf(self, value)\n",
      " |      Compute the log of the cumulative distribution function for Normal distribution\n",
      " |      at the specified value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value: numeric\n",
      " |          Value(s) for which log CDF is calculated. If the log CDF for multiple\n",
      " |          values are desired the values must be provided in a numpy array or theano tensor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      TensorVariable\n",
      " |  \n",
      " |  logp(self, value)\n",
      " |      Calculate log-probability of Normal distribution at specified value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : numeric\n",
      " |          Value(s) for which log-probability is calculated. If the log probabilities for multiple\n",
      " |          values are desired the values must be provided in a numpy array or theano tensor\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      TensorVariable\n",
      " |  \n",
      " |  random(self, point=None, size=None)\n",
      " |      Draw random values from Normal distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      point : dict, optional\n",
      " |          Dict of variable values on which random values are to be\n",
      " |          conditioned (uses default point if not specified).\n",
      " |      size : int, optional\n",
      " |          Desired size of random sample (returns one sample if not\n",
      " |          specified).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __latex__ = _repr_latex_(self, name=None, dist=None)\n",
      " |      Magic method name for IPython to use for LaTeX formatting.\n",
      " |  \n",
      " |  default(self)\n",
      " |  \n",
      " |  get_test_val(self, val, defaults)\n",
      " |  \n",
      " |  getattr_value(self, val)\n",
      " |  \n",
      " |  logp_nojac(self, *args, **kwargs)\n",
      " |      Return the logp, but do not include a jacobian term for transforms.\n",
      " |      \n",
      " |      If we use different parametrizations for the same distribution, we\n",
      " |      need to add the determinant of the jacobian of the transformation\n",
      " |      to make sure the densities still describe the same distribution.\n",
      " |      However, MAP estimates are not invariant with respect to the\n",
      " |      parametrization, we need to exclude the jacobian terms in this case.\n",
      " |      \n",
      " |      This function should be overwritten in base classes for transformed\n",
      " |      distributions.\n",
      " |  \n",
      " |  logp_sum(self, *args, **kwargs)\n",
      " |      Return the sum of the logp values for the given observations.\n",
      " |      \n",
      " |      Subclasses can use this to improve the speed of logp evaluations\n",
      " |      if only the sum of the logp values is needed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  dist(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __new__(cls, name, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(pm.Normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PyMC3 module, the structure for probability distributions looks like this:\n",
    "\n",
    "pymc3.distributions - https://docs.pymc.io/api/distributions.html\n",
    "* continuous - https://docs.pymc.io/api/distributions/continuous.html\n",
    "* discrete - https://docs.pymc.io/api/distributions/discrete.html\n",
    "* timeseries - https://docs.pymc.io/api/distributions/timeseries.html\n",
    "* mixture - https://docs.pymc.io/api/distributions/mixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Discrete',\n",
       " 'Distribution',\n",
       " 'Iterable',\n",
       " 'Mixture',\n",
       " 'Normal',\n",
       " 'NormalMixture',\n",
       " '_DrawValuesContext',\n",
       " '_DrawValuesContextBlocker',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_conversion_map',\n",
       " 'all_discrete',\n",
       " 'bound',\n",
       " 'broadcast_distribution_samples',\n",
       " 'draw_values',\n",
       " 'generate_samples',\n",
       " 'get_tau_sigma',\n",
       " 'get_variable_name',\n",
       " 'logsumexp',\n",
       " 'np',\n",
       " 'random_choice',\n",
       " 'theano',\n",
       " 'to_tuple',\n",
       " 'tt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pm.distributions.mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unobserved Random Variables¶\n",
    "Every unobserved RV has the following calling signature: name (str), parameter keyword arguments. Thus, a normal prior can be defined in a model context like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the model, we can evaluate its logp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0.91893853)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.logp({'x': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observed Random Variables¶\n",
    "Observed RVs are defined just like unobserved RVs but require data to be passed into the observed keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    obs = pm.Normal('x', mu=0, sigma=1, observed=np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observed supports lists, numpy.ndarray, theano and pandas data structures.\n",
    "\n",
    "# Deterministic transforms¶\n",
    "PyMC3 allows you to freely do algebra with RVs in all kinds of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    y = pm.Gamma('y', alpha=1, beta=1)\n",
    "    plus_2 = x + 2\n",
    "    summed = x + y\n",
    "    squared = x**2\n",
    "    sined = pm.math.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these transformations work seamlessly, their results are not stored automatically. Thus, if you want to keep track of a transformed variable, you have to use pm.Deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    plus_2 = pm.Deterministic('x plus 2', x + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that plus_2 can be used in the identical way to above, we only tell PyMC3 to keep track of this RV for us.\n",
    "\n",
    "# Automatic transforms of bounded RVs¶\n",
    "In order to sample models more efficiently, PyMC3 automatically transforms bounded RVs to be unbounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform('x', lower=0, upper=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the RVs of the model, we would expect to find x there, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x_interval__]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.free_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_interval__ represents x transformed to accept parameter values between -inf and +inf. In the case of an upper and a lower bound, a LogOdds transform is applied. Sampling in this transformed space makes it easier for the sampler. PyMC3 also keeps track of the non-transformed, bounded parameters. These are common determinstics (see above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
